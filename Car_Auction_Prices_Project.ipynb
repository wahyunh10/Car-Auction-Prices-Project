{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Car Auction Prices Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6EZkh1ZXA+bJ8NW7oURNR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wahyunh10/Car-Auction-Prices-Project/blob/main/Car_Auction_Prices_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Car Auction Prices Project**"
      ],
      "metadata": {
        "id": "yKhrbC_qm4J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this project, there are 4 steps that I do:**\n",
        "\n",
        "1. Data Preparation (Data Cleaning)\n",
        "  * Extract Raw Data\n",
        "  * Fill and Drop Null Values\n",
        "  * Standardization Values\n",
        "  * Load Clean Data to CSV and Local MySQL Database\n",
        "2. Exploration Data Analysis\n",
        "  * Find Insight using Matplotlib and Seaborn\n",
        "  * Outlier Analysis\n",
        "  * Find Best and Worst Seller using Quantile Analysis\n",
        "3. Machine Learning Modelling\n",
        "  * Prediction Prices Cars using Regression\n",
        "  * Clustering Quality of Cars using KMeans Clustering\n",
        "4. Hypothesis Testing using MannWhitneyU"
      ],
      "metadata": {
        "id": "bMe6_PsoocaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains historical car auction sales prices, scraped from the outside internet sources collected in 2015. This dataset taken from kaggle (https://www.kaggle.com/tunguz/used-car-auction-prices)."
      ],
      "metadata": {
        "id": "qMuGyksyo34q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains 491641 rows and 15 columns:\n",
        "\n",
        "* year : information of year production of the car\n",
        "* make : brand of the car\n",
        "* model : model of the brand car\n",
        "* trim : spesific model of the car\n",
        "* body : body type of the car\n",
        "* transmission : transmission type of the car\n",
        "* vin : unique id of the car\n",
        "* state : code state where the transaction happened\n",
        "* condition : rating condition of the car\n",
        "* odometer : total distance of the car\n",
        "* color : body color of the car\n",
        "* interior : interior color of the car\n",
        "* seller : place that sold the car\n",
        "* mmr : market price of the car\n",
        "* sellingprice : price of the car\n",
        "* saledate : date when the car sold"
      ],
      "metadata": {
        "id": "whXzS5CAo6yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "E4u2NqEet2rJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3ca7QtQHm1KU"
      },
      "outputs": [],
      "source": [
        "# Import needed libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import raw data from csv**"
      ],
      "metadata": {
        "id": "u-jeL3XKt9tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_source = pd.read_csv(\".csv\",sep='\\t')\n",
        "df_source.head()"
      ],
      "metadata": {
        "id": "Wh-FJGdjuBVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check length of every row in dataset\n",
        "idx = []\n",
        "for i in range (df_source.shape[0]):\n",
        "    l = len(df_source.iloc[i][0].split(','))\n",
        "    idx.append(l)\n",
        "    \n",
        "# Give temporary name columns \n",
        "tempcol = []\n",
        "for i in range((max(idx))):\n",
        "    tempcol.append(i)\n",
        "\n",
        "df_csv = pd.read_csv(\"car_prices.csv\",names=tempcol) \n",
        "\n",
        "# Extract first row value to make it into columns name\n",
        "col = []\n",
        "tempcol = ['temp1','temp2']\n",
        "for i in df_csv.iloc[0].values:\n",
        "    if str(i) != 'nan':\n",
        "        col.append(i)\n",
        "col.extend(tempcol) \n",
        "\n",
        "df = df_csv.drop(0,axis=0)\n",
        "df.columns=col\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "X2C1jrTTEU7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check data after columns problem solved**"
      ],
      "metadata": {
        "id": "en09n5OkEXnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_info():\n",
        "    listkolom=[]\n",
        "    for i in (df.columns):\n",
        "        listkolom.append(i)\n",
        "\n",
        "    listtipe=[]\n",
        "    for i in (df.columns):\n",
        "        listtipe.append(df[i].dtypes)\n",
        "\n",
        "    nullmany = df.isnull().sum().values\n",
        "\n",
        "    nullpctx = []\n",
        "    for i in nullmany:\n",
        "        nullpctx.append(round(i/len(df),4)*100)\n",
        "\n",
        "    unik = df.nunique().values\n",
        "\n",
        "    samp = []\n",
        "    for i in df.columns:\n",
        "        samp.append(df[i].sample().values)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'dataFeatures' : listkolom,\n",
        "        'dataType' : listtipe,\n",
        "        'null' : nullmany,\n",
        "        'nullpct' : nullpctx,\n",
        "        'unique' : unik,\n",
        "        'SampleValue' : samp\n",
        "    })\n",
        "\n",
        "data_info()"
      ],
      "metadata": {
        "id": "i1KiEZc8EYY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> It looks that data successfully stored into dataframe, and only have 21 anomaly rows that stored in temp1 column. Column temp2 is empty and will be deleted later.\n",
        "\n"
      ],
      "metadata": {
        "id": "jf8Dc-wIEdRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process to tackle columns overlapping problem**"
      ],
      "metadata": {
        "id": "bTTQMYcCE2GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make all value has lowercase word\n",
        "def LowerVal(col):\n",
        "    newval = []\n",
        "    for i in df[col].values:\n",
        "        if str(i)=='nan':\n",
        "            newval.append(np.nan)\n",
        "        else:\n",
        "            cap = str(i).lower()\n",
        "            newval.append(cap)\n",
        "    df[col]=newval"
      ],
      "metadata": {
        "id": "elZPnyPGEaOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to all objects columns\n",
        "col_to_clean = df.select_dtypes(include='object').columns\n",
        "for i in col_to_clean:\n",
        "    LowerVal(i)"
      ],
      "metadata": {
        "id": "IVQXC2Z0E5sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix some value in 'make' columns\n",
        "makeFix = []\n",
        "for i in range(df.shape[0]):\n",
        "    if str(df['make'][i]) == 'nan':\n",
        "        makeFix.append(np.nan)\n",
        "    else:\n",
        "        newMake = df['make'][i].split()[0]\n",
        "        makeFix.append(newMake)\n",
        "\n",
        "df['make'] = makeFix\n",
        "\n",
        "df.loc[df[df['make']=='Mercedes-b'].index,'make'] = 'Mercedes-benz'\n",
        "df.loc[df[df['make']=='Mercedes'].index,'make'] = 'Mercedes-benz'\n",
        "df.loc[df[df['make']=='Land'].index,'make'] = 'Landrover'\n",
        "df.loc[df[df['make']=='Chev'].index,'make'] = 'Chevrolet'"
      ],
      "metadata": {
        "id": "ZTG9E-nuE7ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['transmission'].value_counts()"
      ],
      "metadata": {
        "id": "6gEkoqVtE9Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> From transmission column value, we can take conclusion rows that have 'sedan' as it value is the anomaly rows."
      ],
      "metadata": {
        "id": "pyG_kta0FB5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take rows that have 'sedan' as it value\n",
        "enum=list(enumerate(df['transmission']=='sedan'))\n",
        "idx_inv = []\n",
        "for idx,val in enum:\n",
        "    if val == True:\n",
        "        idx_inv.append(idx)\n",
        "        \n",
        "inv_data = df.iloc[idx_inv]\n",
        "\n",
        "# drop body column in invalid dataframe\n",
        "inv_data.drop(['body'],1,inplace=True)\n",
        "inv_data.head()"
      ],
      "metadata": {
        "id": "j5IoTRdJFE0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> body column in invalid dataframe dropped because it contain wrong information"
      ],
      "metadata": {
        "id": "ZHI-Ebu9FGrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['temp2'],1,inplace=True)\n",
        "inv_data.columns=df.columns\n",
        "\n",
        "# Re-fill value for invalid rows\n",
        "newval = []\n",
        "for i in range (inv_data.shape[0]):\n",
        "    listedval = list(inv_data.iloc[i].values)\n",
        "    newval.append(listedval)\n",
        "    \n",
        "df.iloc[idx_inv] = newval\n",
        "df.drop(['temp1'],1,inplace=True)\n",
        "\n",
        "# Check dataframe after problem invalid value rows solved \n",
        "df.info()"
      ],
      "metadata": {
        "id": "1niPSepVFHnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning Process**"
      ],
      "metadata": {
        "id": "znOM-mqkFVKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check total null values in every column\n",
        "pd.isna(df).sum()"
      ],
      "metadata": {
        "id": "eHKgektOFiOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check and replace if there are '-' value in dataset.\n",
        "def StripVal(col):\n",
        "    newval = []\n",
        "    for i in df[col].values:\n",
        "        if str(i)=='—':\n",
        "            newval.append(np.nan)\n",
        "        else:\n",
        "            newval.append(i)\n",
        "    df[col]=newval"
      ],
      "metadata": {
        "id": "IPh9EXlOFkRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply StripVal function to every columns object\n",
        "col_obj = df.select_dtypes('object').columns\n",
        "for i in col_obj:\n",
        "    StripVal(i)"
      ],
      "metadata": {
        "id": "19RZgEL2Fki5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to drop selected rows\n",
        "def drop_val(col):\n",
        "    idx_to_drop = []\n",
        "    for i in col.index:\n",
        "        idx_to_drop.append(i)\n",
        "\n",
        "    df.drop(idx_to_drop,inplace=True)\n",
        "    df.reset_index(drop=True,inplace=True)"
      ],
      "metadata": {
        "id": "KDxhO-ZzFl7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the car have no information about all of make, model, body, and trim columns so we cannot describe what car is it.\n",
        "\n",
        "dataNull = df[(df['make'].isna()) & (df['model'].isna()) & (df['body'].isna()) & (df['trim'].isna())]\n",
        "\n",
        "drop_val(dataNull)"
      ],
      "metadata": {
        "id": "EvKClqxbFodP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the car does not have color or interior information, dropped because it haven't got enough information.\n",
        "\n",
        "data_no_color = df[(df['color'].isna()) | (df['interior'].isna())]\n",
        "\n",
        "drop_val(data_no_color)"
      ],
      "metadata": {
        "id": "rhkIjZLlFo-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change data type to number\n",
        "\n",
        "df['year'] = df['year'].astype('int')\n",
        "df['mmr'] = df['mmr'].astype('int')\n",
        "df['sellingprice'] = df['sellingprice'].astype('int')\n",
        "df['condition'] = df['condition'].astype('float')\n",
        "df['odometer'] = df['odometer'].astype('float')"
      ],
      "metadata": {
        "id": "FrVL2MB1Fqws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixing date columns\n",
        "\n",
        "fixed_date = []\n",
        "for i in range(len(df['saledate'])):\n",
        "    date = df.saledate[i][:15]\n",
        "    fixed_date.append(date)\n",
        "    \n",
        "df['saledate'] = fixed_date\n",
        "df['saledate'] = pd.to_datetime(df['saledate'])\n",
        "\n",
        "date_not_use = df[(df['saledate'] < '2014-12-01')]  # Deleting this data because there aren't enough data for that range of date \n",
        "drop_val(date_not_use)                              # It can cause trouble to EDA and Machine Learning modelling"
      ],
      "metadata": {
        "id": "xcaAdSXJFss1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "wJA56rTKFwM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This table above show us statistical descriptive of the data, it give insight that some columns have unusual and potentially wrong data. For example:\n",
        "\n",
        "* odometer have min and max value too far from the quartile, make the spread of data become ugly and have too large outlier\n",
        "* mmr have minimal value so little that have to check again\n",
        "* sellingprice have min and max value too far from the quartile like odometer column."
      ],
      "metadata": {
        "id": "2Mlt8KaOFzZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing Sellingprice column\n",
        "\n",
        "# Change column that have maximum sellingprice because its value is too big\n",
        "# After some check, there is only one column that has maximum sellingprice value, that column have 22800 mmr\n",
        "wr_val_sellingprice = df[df['sellingprice']==df['sellingprice'].max()].index\n",
        "\n",
        "# Because the value of mmr and sellingprice is usually not too far, so I remove one '0' at this cell of sellingprice column\n",
        "df.loc[wr_val_sellingprice,['sellingprice']] = 23000  \n",
        "\n",
        "# Delete data that have sellingprice below 100, because it doesn't make sense\n",
        "inv_sellingprice = df[df['sellingprice']<100]\n",
        "drop_val(inv_sellingprice)"
      ],
      "metadata": {
        "id": "nFjPjkh5F5c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing mmr column\n",
        "\n",
        "# Delete null values or mmr values that have values below 100\n",
        "inv_mmr = df[(df['mmr'].isna()) | (df['mmr']<100)]\n",
        "drop_val(inv_mmr)\n",
        "\n",
        "# I dropped it because mmr value have the exact number at some website for that year, \n",
        "# so if its null, it can't be filled by any information in this dataset\n",
        "# mmr under 100 dropped because it just doesn't make sense for used cars have mmr price under 100"
      ],
      "metadata": {
        "id": "zL5sVmjfF-uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing odometer column\n",
        "\n",
        "# Delete odometer that have value below 10.0 or above 9000000\n",
        "odometer_inv = df[(df['odometer']<10.0) | (df['odometer']>900000.0)]\n",
        "drop_val(odometer_inv)\n",
        "\n",
        "# Dropped because it is sure that almost every car doesn't have that value in its odometer"
      ],
      "metadata": {
        "id": "d49tmLFGGAbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Show Correlation Between Columns to Get More Insight**"
      ],
      "metadata": {
        "id": "-XPTFhkbGCCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(df.corr(),linewidth=0.5,annot=True)\n",
        "plt.title('Correlation Between Numerical Columns',fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FCLC0g29GCy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the heatmap, we can conclude that:\n",
        "\n",
        "* year and odometer have high negative correlation, it means higher the year so lower the odometer\n",
        "* mmr and sellingprice have high positive correlation, it means higher the mmr so sellingprice is high too\n",
        "* other columns have a little correlation with each other since its value is only between 0.4 to 0.6"
      ],
      "metadata": {
        "id": "xMkSy0IBGGGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check again numerical columns after fixing some columns\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "ilFAGAl5GLzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping condition is null because there are no good ways to fill it\n",
        "condition_null = df[(df['condition'].isna())]\n",
        "drop_val(condition_null)"
      ],
      "metadata": {
        "id": "uHoT4wtSGNsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill Odometer \n",
        "# Its filled value taken from median odometer per year\n",
        "medianOdoByYear= df.groupby('year').median()['odometer']\n",
        "def fillOdo(x):\n",
        "    forfill = []\n",
        "    for i in df['year']:\n",
        "        if x['odometer'] > 0:\n",
        "            return x['odometer']\n",
        "        else:\n",
        "            if x['year'] > 0:\n",
        "                return medianOdoByYear.loc[x['year']]\n",
        "            else:\n",
        "                return np.nan\n",
        "df['odometer'] = df.apply(fillOdo,axis=1)"
      ],
      "metadata": {
        "id": "CC2xSUquGPk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fill 'model' Column**"
      ],
      "metadata": {
        "id": "qt4gdfk6GRQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take null value from model column\n",
        "modelNull = df[df['model'].isna()]\n",
        "\n",
        "# Check the similiarity from make and trim columns in rows that have null value of models\n",
        "model_trim = []\n",
        "temptrim = []\n",
        "for i in modelNull['make'].unique():\n",
        "    for j in modelNull['trim'].unique():\n",
        "        model_trim.append(i)\n",
        "        try:\n",
        "            cek = df[(df['make']==str(i)) & (df['trim']==str(j)) & (df['body']=='sedan')]['model'].value_counts().index[0]\n",
        "            temptrim.append(cek)\n",
        "        except:\n",
        "            temptrim.append(np.nan)\n",
        "            \n",
        "print(temptrim)\n",
        "print(model_trim)"
      ],
      "metadata": {
        "id": "tjEbI1WNGTcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_del = []\n",
        "idx_to_change = []\n",
        "ls_7 = []\n",
        "for idx,val in list(enumerate(df['model'])):\n",
        "    if str(val) == 'nan':\n",
        "        idx_to_change.append(idx)\n",
        "        \n",
        "        for item in df[df['make'] == 'audi'].index:\n",
        "            idx_del.append(item)            \n",
        "\n",
        "# Make temporary dataframe for rows to filled         \n",
        "tempdf = df.iloc[idx_to_change]\n",
        "\n",
        "# Find cars that have '7' in their trim\n",
        "for item in modelNull['trim'].unique():\n",
        "    if re.search('7.+',item):\n",
        "        ls_7.append(item)\n",
        "\n",
        "# Filled model that have '7' in their trim to '7 Series'\n",
        "for i in ls_7:\n",
        "    df.loc[tempdf[tempdf['trim']==str(i)].index,'model'] = '7 series'            \n",
        "\n",
        "# Filled model that have '650i xdrive' in their trim to '6 series gran coupe'\n",
        "df.loc[tempdf[tempdf['trim']=='650i xdrive'].index,'model'] = '6 series gran coupe'   \n",
        "\n",
        "# Dropped because there are no value trim when make is 'audi' \n",
        "df.drop(idx_del,inplace=True)\n",
        "\n",
        "# Dropped if there are still null values\n",
        "model_null_idx = df[df['model'].isna()]\n",
        "drop_val(model_null_idx)"
      ],
      "metadata": {
        "id": "YV504k1vGWGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fill 'trim' Column**"
      ],
      "metadata": {
        "id": "jiGBrTiRGYVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trimNotNull = df.loc[df['trim'].notnull()][['make', 'model', 'trim']]\n",
        "trimNotNull['combined'] = trimNotNull['make'].str.cat(trimNotNull['model'].astype(str), sep=\"_\")\n",
        "trimNotNull.drop(['make', 'model'],1,inplace=True)"
      ],
      "metadata": {
        "id": "5ALpPGQFGX-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['combined'] = df['make'].str.cat(df['model'].astype(str), sep=\"_\")\n",
        "trimGroupby = trimNotNull.groupby('combined').first()['trim']\n",
        "\n",
        "def fillTrim(x):\n",
        "    for i in df['combined']:\n",
        "        if str(x['trim']) != 'nan':\n",
        "            return x['trim']\n",
        "        else:\n",
        "            try:\n",
        "                return trimGroupby.loc[x['combined']]\n",
        "            except:\n",
        "                return np.nan\n",
        "            \n",
        "df['trim'] = df.apply(fillTrim,axis=1)\n",
        "df.drop(['combined'],1,inplace=True)\n",
        "\n",
        "trim_null_idx = df[df['trim'].isna()]\n",
        "drop_val(trim_null_idx)"
      ],
      "metadata": {
        "id": "LBBmmUtNGbdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}